<!DOCTYPE html>
<html lang="en-US"> <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Risks &amp; Challenges | Ethics of Self-Driving Cars</title> <meta name="generator" content="Jekyll v4.3.2"> <meta property="og:title" content="Risks &amp; Challenges"> <meta property="og:locale" content="en_US"> <meta name="description" content="COMP501 Assignment 3"> <meta property="og:description" content="COMP501 Assignment 3"> <link rel="canonical" href="a30812.github.io/risks/"> <meta property="og:url" content="a30812.github.io/risks/"> <meta property="og:site_name" content="Ethics of Self-Driving Cars"> <meta property="og:type" content="website"> <meta name="twitter:card" content="summary"> <meta property="twitter:title" content="Risks &amp; Challenges"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"COMP501 Assignment 3","headline":"Risks &amp; Challenges","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"a30812.github.io/assets/0812.svg"}},"url":"a30812.github.io/risks/"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewbox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewbox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewbox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-github" viewbox="0 0 16 16"> <title>GitHub</title> <svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" fill-rule="evenodd" aria-hidden="true" version="1.1" data-view-component="true"> <path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"></path> </svg> </symbol> <symbol id="svg-jekyll" viewbox="0 0 16 16"> <title>Jekyll</title> <svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" fill-rule="evenodd" aria-hidden="true" version="1.1" data-view-component="true"> <path id="tube" d="M8.8929329,0.1342923c0.4068098,0,1.003397,0.1283112,1.6369591,0.3719619 c0.5779533,0.2222824,1.0683889,0.4951643,1.3793039,0.7483078c0.3536234,0.2544186,0.5434761,0.4987274,0.4796448,0.6647159 c-0.0019951,0.0052563-0.0049114,0.0100911-0.0074062,0.0151531l-0.0022612,0.0046837 c-0.0137377,0.0257068-0.0338793,0.0484217-0.0598555,0.06822c0,0-0.5177536,0.457515-0.6085358,0.6936777L7.0679126,14.7732782 l-0.0004215-0.0001917c-0.2680159,0.677495-0.917964,1.0926218-1.6069012,1.0926218 c-0.2061186,0-0.41576-0.0371695-0.6196656-0.1155882c-0.1859088-0.0715055-0.3520551-0.1719303-0.495954-0.2937212 c-0.6375384-0.4397087-0.9187899-1.2702904-0.6363606-2.0261593l-0.0004244-0.0001535l4.64291-12.0721893 c0.1092749-0.2841628,0.0130453-0.9227035,0.0130453-0.9227035C8.3581171,0.403117,8.3583078,0.3727289,8.365406,0.3444508 l0.00142-0.0050643c0.0014973-0.0054109,0.0026484-0.0109354,0.0046816-0.0161929 C8.4204912,0.1958203,8.6111917,0.1342923,8.8929329,0.1342923 M8.8929329,0C8.5339413,0,8.3163347,0.0925203,8.2461653,0.2749932 C8.241951,0.2858829,8.2394629,0.2955653,8.2379751,0.3013521L8.2373924,0.3035874L8.2360973,0.3082056l-0.000494,0.0017681 l-0.0004482,0.001781c-0.0115004,0.0458152-0.0126772,0.0947545-0.003499,0.1455146 c0.0253191,0.1698864,0.0727901,0.6477911-0.0059042,0.8524263L3.5828424,13.3818798L3.533653,13.509779l0.0062423,0.0022573 c-0.2244573,0.7645073,0.0616043,1.5913115,0.7233822,2.0511398c0.1582155,0.1329527,0.3363028,0.2380009,0.529438,0.3122845 C5.0075908,15.9581003,5.232295,16,5.4605894,16c0.3693519,0,0.7275577-0.108882,1.0358973-0.3148775 c0.2761874-0.184515,0.4950714-0.4362526,0.6391649-0.7335405l0.006021,0.0027361l0.0515828-0.1328335l4.642869-12.0722656 c0.0572281-0.1488764,0.377821-0.4691439,0.5688782-0.6383862c0.0395842-0.0310204,0.0707293-0.0674739,0.0926104-0.1084204 l0.001297-0.0024275l0.0011969-0.0024766l0.0022612-0.0046837l0.0003595-0.0006088 c0.0027657-0.005247,0.0073929-0.014029,0.0116644-0.0252829c0.0868893-0.2259369-0.0940409-0.5092824-0.5235062-0.8190312 c-0.3328915-0.2701689-0.8475275-0.5495882-1.4127874-0.7669901C9.9486284,0.1388367,9.3344116,0,8.8929329,0L8.8929329,0z"></path> <path id="liquid" d="M9.8217888,5.8058114c0,0-0.643261,0.7422543-1.2257833,1.0011692 c-0.5825596,0.2589149-1.0152106,0.1942253-1.5398345,0.500833C6.5315495,7.6144605,6.2643461,8.0454636,6.2643461,8.0454636 l-2.1393914,5.5628481c-0.2348194,0.6286802,0.1102343,1.3623743,0.7477388,1.6075544s1.3561592-0.0794621,1.6031051-0.7034235 l-0.0021873-0.0008059L9.8217888,5.8058114z M7.7511539,8.995369c0.1119933,0,0.2027807,0.0907898,0.2027807,0.2027826 S7.8631473,9.4009323,7.7511539,9.4009323c-0.1119928,0-0.2027826-0.0907879-0.2027826-0.2027807 S7.6391611,8.995369,7.7511539,8.995369z M6.8693128,8.0882444c0.074296,0,0.134522,0.0602283,0.134522,0.1345224 c0,0.074295-0.060226,0.1345234-0.134522,0.1345234c-0.0742936,0-0.134522-0.0602283-0.134522-0.1345234 C6.7347908,8.1484728,6.7950191,8.0882444,6.8693128,8.0882444z M5.7889886,11.2644882 c-0.1120143,0-0.2028203-0.090806-0.2028203-0.2028198s0.090806-0.2028198,0.2028203-0.2028198 c0.1120138,0,0.2028198,0.090806,0.2028198,0.2028198S5.9010024,11.2644882,5.7889886,11.2644882z M6.0245385,10.3581305 c0-0.2101059,0.1703243-0.3804302,0.3804302-0.3804302s0.3804302,0.1703243,0.3804302,0.3804302 c0,0.2101049-0.1703243,0.3804321-0.3804302,0.3804321S6.0245385,10.5682354,6.0245385,10.3581305z M6.5968523,12.4317179 c0.074296,0,0.134522,0.0602274,0.134522,0.1345234c0,0.0742941-0.060226,0.1345224-0.134522,0.1345224 c-0.0742936,0-0.134522-0.0602283-0.134522-0.1345224C6.4623303,12.4919453,6.5225587,12.4317179,6.5968523,12.4317179z"></path> <ellipse id="top" transform="matrix(0.3589791 -0.9333456 0.9333456 0.3589791 5.5751348 10.3211546)" class="st0" cx="10.3015223" cy="1.1017959" rx="0.3395875" ry="1.2911236"></ellipse> </svg> </symbol> <symbol id="svg-doc"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewbox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewbox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewbox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"></path> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"></path> </svg> </symbol> <symbol id="svg-copied" viewbox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewbox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"></path> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"></path> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> <div class="site-logo" role="img" aria-label="Ethics of Self-Driving Cars"></div> </a> <button id="menu-button" class="site-button" aria-label="Open menu"> <svg viewbox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg>  </button>
</div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list">
<li class="nav-list-item"><a href="/" class="nav-list-link">Introduction</a></li>
<li class="nav-list-item"><a href="/opportunities/" class="nav-list-link">Opportunities</a></li>
<li class="nav-list-item active"><a href="/risks/" class="nav-list-link active">Risks &amp; Challenges</a></li>
<li class="nav-list-item"><a href="/choices/" class="nav-list-link">Choices</a></li>
<li class="nav-list-item">
<a href="#" class="nav-list-expander" aria-label="toggle links in Ethics category"> <svg viewbox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="/ethics/" class="nav-list-link">Ethics</a><ul class="nav-list">
<li class="nav-list-item "><a href="/ethics/dylan/" class="nav-list-link">Dylan's Reflections</a></li>
<li class="nav-list-item "><a href="/ethics/matt/" class="nav-list-link">Matt's Reflections</a></li>
<li class="nav-list-item "><a href="/ethics/thomas/" class="nav-list-link">Thomas' Reflections</a></li>
</ul>
</li>
<li class="nav-list-item"><a href="/references/" class="nav-list-link">References</a></li>
<li class="nav-list-item"><a href="/minutes/" class="nav-list-link">Meeting Minutes</a></li>
</ul> </nav> <footer class="site-footer"> <a href="https://github.com/A30812/a30812.github.io" class="icon" alt="" rel="external" target="_blank"> <svg aria-hidden="true"><use href="#svg-github"></use></svg> </a> <a href="https://jekyllrb.com" class="icon" rel="external" target="_blank"> <svg aria-hidden="true"><use href="#svg-jekyll"></use></svg> </a> <a href="https://github.com/A30812/documentation" class="icon" rel="external" target="_blank"> <svg aria-hidden="true"><use href="#svg-doc"></use></svg> </a> <div>Built with <a href="https://github.com/just-the-docs/just-the-docs" class="link-hover" rel="nofollow" target="_blank">Just the Docs</a>
</div> ©2023 <a href="https://www.github.com/A30812/" class="link-hover" rel="nofollow" target="_blank">0812</a> </footer> </div> <div class="main" id="top"> <div class="text-header">Ethics of Self-Driving Cars: Safety and Liability</div> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Ethics of Self-Driving Cars" aria-label="Search Ethics of Self-Driving Cars" autocomplete="off"> <label for="search-input" class="search-label"><svg viewbox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div id="main-content-wrap" class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 id="risk-and-challenges"> <a href="#risk-and-challenges" class="anchor-heading" aria-labelledby="risk-and-challenges"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Risk and Challenges </h1> <h2 id="transition-period"> <a href="#transition-period" class="anchor-heading" aria-labelledby="transition-period"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Transition Period </h2> <p>The transitory period when self-driving cars are first unleashed on public roads marks one of the greatest potential risk periods. At this time, self-driving cars will be working from relatively small datasets of ‘real-world’ scenarios (Dickson, 2020) and no amount of public messaging is going to ensure that every ‘Joe Public’ is going to respond to these new cars in a sensible manner.</p> <p>“It takes a long time to turn over the U.S. fleet of light-duty vehicles, with the average vehicular age currently being 11.4 years.” Therefore, “there will likely be at least a several-decade-long period during which conventional and self-driving vehicles would need to interact.” (Sivak &amp; Schoettle, 2015)</p> <p>During this transition period, it is important to acknowledge the risks of the coexistence between self-driving cars and conventional vehicles, as safety may worsen. Sivak and Schoettle (2015) state “in many current situations, interacting drivers of conventional vehicles make eye contact and proceed according to the feedback received from other drivers. Such feedback would be absent in interactions with self-driving vehicles.”</p> <p>Dickson (2020) argues that we will not get Level 5 vehicles via deep learning as they must deal with human drivers still on the road, though he fails to consider that the transition period will not last forever. He primarily considers Tesla as an example for self-driving vehicles, failing to account for the other companies who are working on self-driving technology.<br> Furthermore, Dickson takes the viewpoint that deep learning is not akin to the human mind, instead being “fundamentally flawed because it can only interpolate.”<br> Though this may indeed be the case, and would support Sivak and Schoettle’s absent feedback argument, the opaque nature of deep learning models cannot currently prove Dickson and those who agree with him correct.</p> <h2 id="deep-learning-models-are-opaque-in-their-decision-making"> <a href="#deep-learning-models-are-opaque-in-their-decision-making" class="anchor-heading" aria-labelledby="deep-learning-models-are-opaque-in-their-decision-making"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Deep Learning Models are Opaque in Their Decision-making </h2> <p>Deep learning models, like those used in AI, are inherently opaque. Engineers may understand what the car sensed and how the car responded to a situation, but in many implementations, crucial aspects of decision-making seem to occur in a ‘black box’. (Stilgoe, 2021).</p> <p>This is in sharp contrast to the state-based and deterministic automated systems to which we are accustomed. When these types of systems fail a decision-making process, the failure can be identified, fixed, and tested with a high degree of confidence. When a deep learning model fails, this is usually indicative of an alignment issue.</p> <p>Alignment issues typically arise when the deep learning model learns incorrectly, such as learning the wrong goal, and thus arrives at the incorrect outcome. This is known as an “out-of-distribution generalisation” failure, where a deep learning model should perform “well on test data that is not distributed identically to the training set”, yet fails (Langosco et al., 2023).</p> <p>This type of alignment issue “is a fundamental problem in machine learning” (Langosco et al., 2023) as it cannot be reasonably assumed that the model can distinguish between its “terminal goal” and its “instrumental goal” (Miles, 2021). An out-of-distribution failure could therefore cause safety issues - or even something as simple as making passengers late - with self-driving cars.</p> <p>It is worth noting that whilst deep learning decision-making isn’t entirely opaque, as their learning processes can be observed, there is a lack of understanding on why they come to certain conclusions about their objectives based on their teachings, thus potentially making them dangerous.</p> <h2 id="ethical-dilemmas--the-trolley-problem"> <a href="#ethical-dilemmas--the-trolley-problem" class="anchor-heading" aria-labelledby="ethical-dilemmas--the-trolley-problem"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Ethical Dilemmas &amp; the Trolley Problem </h2> <p>Addressing the problem of AI ethics will be critical for public acceptance of self-driving cars, and failure to properly align AI decision-making may have serious long-term consequences for the self-driving industry.<br> The question of ethics is cast into its sharpest relief when introducing vehicle AI to dilemmas: scenarios in which, no matter which course the car takes, some harm is inevitable. This represents a new frontier for humanity, as never before have we tasked machines with valuing human life to this degree.</p> <p>The MIT launched ‘the Moral Machine experiment’ in June 2016 with the goal of starting discussion and providing moral guidance to self-driving vehicle designers. Survey participants were provided with a birds-eye view of an ethical dilemma and tasked with picking a preferred outcome.<br> The findings demonstrated a strong preference for sparing human lives over those of pets, sparing more people over fewer, sparing the young, and sparing those lawful and of higher status, such as doctors and pregnant women. (Furey &amp; Hill, 2021)</p> <p>These preferences, however, vary significantly by region. For example, countries in the ‘Eastern’ region (broadly speaking, every Eurasian country between Iran and Japan) having a preference towards the elderly. The broad array of correlations between distinct cultures suggests that the question of the machine ethics may not have a one-size-fits-all solution, despite much talk of the need for AI to reflect human values.</p> <p>Furey and Hill (2020) argue that the focus on ‘trolley problem’-style dilemmas may cause more harm than good. Specifically, that by focusing on accidents that represent edge cases, the adoption of AVs may have been slowed, preventing them from saving lives attributed to one of the primary causes of road accidents: human driver error. Furthermore, Furey and Hill (2020) argue it may place unnecessary moral discomfort on the public: “[The Moral Machine] has caused the public to think they must choose between purchasing an autonomous vehicle that protects their family and one that is moral.”</p> <p class="article-image"><img src="/assets/images/trolley_problem.png" alt="Trolley Problem"></p> <h6 id="figure-2-one-of-the-dilemmas-included-in-the-trolley-problem-should-you-pull-the-lever-to-divert-the-runaway-trolley-onto-the-side-track-mcgeddon--zapyon-2018"> <a href="#figure-2-one-of-the-dilemmas-included-in-the-trolley-problem-should-you-pull-the-lever-to-divert-the-runaway-trolley-onto-the-side-track-mcgeddon--zapyon-2018" class="anchor-heading" aria-labelledby="figure-2-one-of-the-dilemmas-included-in-the-trolley-problem-should-you-pull-the-lever-to-divert-the-runaway-trolley-onto-the-side-track-mcgeddon--zapyon-2018"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Figure 2. “One of the dilemmas included in the trolley problem: should you pull the lever to divert the runaway trolley onto the side track?” (McGeddon &amp; Zapyon, 2018) </h6> <p>Some argue that the focus on ‘trolley problem’-style dilemmas is misguided altogether. Human drivers are taught to break without swerving in the case of emergency. This procedure is not recommended merely because it is instinctive and simple, but due to the nature of vehicular physics.<br> “Put in its simplest form, the problem is that swerving sufficiently to avoid an object that is within a car’s stopping distance is always a wildly risky manoeuvre compared to straight-line braking.” (Davnall, 2019)<br> The self-driving trolley problem can now be seen as an additional choice between a controlled manoeuvre with known risks, and an uncontrolled manoeuvre with unknown risks.</p> <h2 id="need-for-new-infrastructure"> <a href="#need-for-new-infrastructure" class="anchor-heading" aria-labelledby="need-for-new-infrastructure"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Need for New Infrastructure </h2> <p>Existing vehicle infrastructure might need to be adapted for self-driving cars. As Othman (2021) states, “the human factor … will not be a concern anymore so the geometric design requirements can be relaxed.” The foremost issue with adapting infrastructure to autonomous vehicles will be the transition period, as we cannot remove all infrastructure for ‘dumb’ cars until the transition to self-driving cars is completed.</p> <p>Whilst infrastructure such as roads can be streamlined during and especially after the transition period, this may increase maintenance of infrastructure. According to Othman (2021), “the decrease in the wheel wander, because of the lane-keeping system, and the increase in the lane capacity, because of the elimination of the human factor, will bring an accelerated rutting potential and will quickly deteriorate the pavement condition.”<br> This could therefore drastically increase costs for infrastructure maintenance, which is already lacking for existing unautomated vehicles. During the transition period, it might make sense to restrict self-driving cars to fixed routes and lanes, as this “allows safe operation during the early phases of driverless mobility” (Bonte, 2021).</p> <p>Bonte (2021) argues that the future of cities could “favour large levels of pedestrianisation with regular traffic moved underground via tunnels and hyperloops”, although this will require a significant investment in infrastructure, which may take many years to implement even after the transition period.</p> <p>“AVs can significantly reduce the number of the required parking lots … as vehicles will be serving customers at different times [and] the autonomous valet parking system will allow vehicles to park closer to each other” (Otham, 2021). This reduction of parking lots may allow us to free up space for other infrastructure such as roads or even housing.</p> <p class="article-image"><img src="/assets/images/parking_strategy_for_vehicles.png" alt="Parking Strategy for Vehicles"></p> <h6 id="figure-3-parking-strategy-for-human-driven-vehicles-and-avs-left-conventional-parking-layout-right-autonomous-vehicles-parking-layout-otham-2021"> <a href="#figure-3-parking-strategy-for-human-driven-vehicles-and-avs-left-conventional-parking-layout-right-autonomous-vehicles-parking-layout-otham-2021" class="anchor-heading" aria-labelledby="figure-3-parking-strategy-for-human-driven-vehicles-and-avs-left-conventional-parking-layout-right-autonomous-vehicles-parking-layout-otham-2021"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Figure 3. Parking Strategy for human-driven vehicles and Avs. (left) Conventional parking layout; (right) Autonomous Vehicles parking layout. (Otham, 2021) </h6> <p>Despite the possible advantages, self-driving cars might have a negative impact on traffic congestion, “because AVs will motivate people to make longer trips, travel further, and make additional trips” (Otham, 2021). Autonomous vehicles therefore will possibly bring disadvantages as well as advantages.</p> <h2 id="need-for-a-new-legal-framework"> <a href="#need-for-a-new-legal-framework" class="anchor-heading" aria-labelledby="need-for-a-new-legal-framework"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Need for a New Legal Framework </h2> <p>Current legal framework is not suitable for handling liability in the event of an accident involving a truly autonomous vehicle. Currently, all self-driving cars on the road have a ‘steward’ or ‘safety-driver’ behind the wheel that can take control if things go wrong and is ultimately responsible for the vehicle’s safe passage. This is a temporary ‘band aid’ solution that eventually needs addressing.</p> <p>Whilst it is conceivable that in the future, the AI that drive vehicles will be agents, with legal rights and responsibilities, this does not represent the immediate reality of self-driving vehicles.</p> <p>With the user giving up control of the vehicle, they have also given up their responsibility for its safe use (Gurney, 2017). This stance is of absolute necessity to the adoption of AVs, as no consumer wants to purchase the ‘Car of Damocles’.</p> <h2 id="public-acceptance"> <a href="#public-acceptance" class="anchor-heading" aria-labelledby="public-acceptance"><svg viewbox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Public Acceptance </h2> <p>Having the technology of self-driving cars up to a good standard is insufficient by itself, as the public’s trust is equally important. When a technology is quite recent and seemingly beyond our control, we are naturally more risk-averse towards it.</p> <p>In a study by Sivak and Schoettle (2015) on the public opinion of self-driving cars, most respondents expressed high levels of concerns about riding in a self-driving car. This is due to the fear/risk of safety issues, malfunction, or system failure. There were also concerns about self-driving cars not performing as well as human drivers.</p> <p>Last year, 378 people were killed on New Zealand’s roads. This, however, is not the number to beat for autonomous vehicle engineers, as polling suggests that the public acceptance of AVs would require this number to be lowered by a factor of 10.</p> <p>This risk aversion poses a risk in and of itself, as an excessively cautious approach resulting from high-profile accidents could potentially hinder the acceptance and development of self-driving cars. Such a hinderance could prevent millions from avoiding life-changing injury or even death.</p> </main> <hr> <footer> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
